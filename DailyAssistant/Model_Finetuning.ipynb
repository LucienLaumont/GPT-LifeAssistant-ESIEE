{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; color:rgb(255, 255, 255);\">Final Project: Building a Daily Life Assistant</h1>\n",
    "\n",
    "<p style=\"font-size: 25px; line-height: 1.6; text-align: justify; max-width: 1200px; margin: 0 auto; margin-bottom: 20px;\">\n",
    "    This project aims to create a practical AI assistant using an instruction fine-tuned GPT-2 model. \n",
    "    The assistant will perform daily tasks such as scheduling, answering questions, and providing personalized recommendations.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size: 20px; line-height: 1.8; max-width: 1000px; margin: 0 auto;\">\n",
    "    <li><strong>Model Architecture & Pretraining</strong>: Understanding GPT-2’s architecture and pretraining process.</li>\n",
    "    <li><strong>Instruction Fine-Tuning</strong>: Training the model with instruction-response pairs for enhanced task performance.</li>\n",
    "    <li><strong>Evaluation & Refinement</strong>: Assessing the model's output and iterating for better results.</li>\n",
    "    <li><strong>Practical Application</strong>: Implementing the model in real-world scenarios such as daily task management.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "We start by loading the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 3)) (2.18.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: torch in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 7)) (3.10.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 8)) (0.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from -r requirements.txt (line 9)) (4.48.0)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 10))\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow->-r requirements.txt (line 3)) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (5.29.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from requests->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from requests->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from requests->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from requests->-r requirements.txt (line 5)) (2024.12.14)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from torch->-r requirements.txt (line 6)) (3.16.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from torch->-r requirements.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from torch->-r requirements.txt (line 6)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from torch->-r requirements.txt (line 6)) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from torch->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 7)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 7)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 7)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from matplotlib->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tiktoken->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (0.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (0.5.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 10))\n",
      "  Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 10))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 10))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lucie\\documents\\cours\\e5\\llm\\gpt-lifeassistant-esiee\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 3)) (0.1.2)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.1 MB 6.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/11.1 MB 9.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/11.1 MB 11.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.3/11.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.1/11.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.1 MB 13.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.7/11.1 MB 14.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.6/11.1 MB 14.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.1 MB 15.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.1 MB 15.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.2/11.1 MB 15.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.0/11.1 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.5/11.1 MB 16.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.6/11.1 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/43.6 MB 20.5 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.7/43.6 MB 21.4 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 2.5/43.6 MB 18.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.4/43.6 MB 18.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 4.2/43.6 MB 18.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 5.0/43.6 MB 17.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.8/43.6 MB 17.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.6/43.6 MB 17.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 7.3/43.6 MB 17.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 8.3/43.6 MB 17.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 9.1/43.6 MB 17.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.8/43.6 MB 17.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.5/43.6 MB 17.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 11.4/43.6 MB 17.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 12.2/43.6 MB 17.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.0/43.6 MB 17.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.8/43.6 MB 17.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 14.5/43.6 MB 16.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.2/43.6 MB 16.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 15.9/43.6 MB 16.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 16.7/43.6 MB 16.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 17.5/43.6 MB 16.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 18.3/43.6 MB 16.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 19.0/43.6 MB 16.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 19.6/43.6 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 20.3/43.6 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 21.1/43.6 MB 16.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 21.9/43.6 MB 16.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 22.6/43.6 MB 15.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.2/43.6 MB 16.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 24.0/43.6 MB 15.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 24.8/43.6 MB 15.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.5/43.6 MB 15.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 26.4/43.6 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 27.0/43.6 MB 15.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 27.7/43.6 MB 16.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 28.4/43.6 MB 15.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 29.2/43.6 MB 16.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 30.0/43.6 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 30.9/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.5/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 32.3/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.1/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 33.9/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.8/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 35.3/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 36.3/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.1/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.8/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.5/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 39.3/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.0/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 40.8/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.6/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.2/43.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.2/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.6/43.6 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucie\\Documents\\Cours\\E5\\llm\\GPT-LifeAssistant-ESIEE\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "from lib_labs.gpt_download import download_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the Pretrained GPT-2 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\")\n",
    "MODEL_SIZE = os.getenv(\"MODEL_SIZE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du modèle gpt2-large dans model...\n",
      "Modèle gpt2-large téléchargé et sauvegardé dans model.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = download_gpt2(MODEL_DIR,MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the cat's food ? Make me a short and clear list of the answers.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What is the cat's food ? Make me a short and clear list of the answers.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\nThe cat's food is the cat's favorite food.\\n\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    attention_mask = attention_mask,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    max_length = 150,\n",
    "    num_beams = 5,\n",
    "    temperature = 1,\n",
    "    top_k = 50,\n",
    "    do_sample = True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Incomplete Responses from GPT-2**\n",
    "\n",
    "In some cases, GPT-2 provides an incomplete response. It starts a sentence but doesn't finish it. This is problematic because it negatively impacts the user experience. There are several solutions to address this issue:\n",
    "\n",
    "1. **Fine-tuning with examples of complete responses**: Train the model on a dataset that includes well-structured and complete answers to improve its behavior.\n",
    "\n",
    "2. **Post-processing generated responses**: Implement logic to analyze the output and request the model to continue if a response is detected as incomplete.\n",
    "\n",
    "3. **Adjusting generation parameters**: Modify parameters such as `max_length`, `temperature`, `top_k`, or `top_p` to increase the likelihood of producing complete and coherent answers.\n",
    "\n",
    "4. **Adding a contextual prefix**: Use a prompt like _\"Please provide a detailed and complete answer:\"_ before the main query to guide the model towards better responses.\n",
    "\n",
    "5. **Automatic verification with a script**: Create a script to detect incomplete responses and prompt the model to continue if necessary.\n",
    "\n",
    "\n",
    "We will choose to integrate a script to handle this issue. Additionally, during the fine-tuning process for a daily assistant, we will ensure that this concern is addressed in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_incomplete_sentences(text):\n",
    "    \"\"\"\n",
    "    Slice the input text into sentences, keeping the formatting \n",
    "    (e.g., \\n, spaces), and remove incomplete phrases that do not \n",
    "    end with a proper punctuation mark.\n",
    "    \"\"\"\n",
    "    # Split the text while keeping the delimiters (e.g., .!?) and formatting\n",
    "    sentences = re.split(r'(?<=[.!?])(\\s+)', text)\n",
    "    \n",
    "    cleaned_text = \"\"\n",
    "    for i in range(0, len(sentences) - 1, 2):  # Process sentences with their trailing spaces\n",
    "        sentence = sentences[i]\n",
    "        trailing_space = sentences[i + 1]\n",
    "        if re.search(r'[.!?]$', sentence):  # Check if the sentence ends with valid punctuation\n",
    "            cleaned_text += sentence + trailing_space\n",
    "    \n",
    "    # Handle cases where the last part is an incomplete sentence\n",
    "    if len(sentences) % 2 != 0 and re.search(r'[.!?]$', sentences[-1]):\n",
    "        cleaned_text += sentences[-1]\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "What is the cat's food ? Make me a short and clear list of the answers.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "\n",
      "\n",
      "Cleaned Text:\n",
      "\n",
      "What is the cat's food ? Make me a short and clear list of the answers.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_complete_sentences = clean_incomplete_sentences(generated_text)\n",
    "\n",
    "print(\"Original Text:\\n\")\n",
    "print(generated_text)\n",
    "print(\"\\nCleaned Text:\\n\")\n",
    "print(text_complete_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Redundant Sentences in GPT-Generated Text**\n",
    "\n",
    "GPT often generates sentences that are almost identical or convey similar information. This redundancy can make it challenging to filter out phrases with overlapping content. To address this issue, we will implement a script that detects and removes duplicate or nearly identical sentences.\n",
    "\n",
    "1. **Sentence Splitting**: \n",
    "   - The text will be divided into individual sentences using a delimiter (e.g., `.`, `!`, `?`).\n",
    "\n",
    "2. **Similarity Detection**:\n",
    "   - We will compare each sentence against others using a similarity metric, such as Levenshtein distance or cosine similarity on vector embeddings.\n",
    "\n",
    "3. **Duplicate Removal**:\n",
    "   - Sentences identified as duplicates or with high similarity scores will be removed, leaving only unique information.\n",
    "\n",
    "4. **Reconstruction**:\n",
    "   - The remaining unique sentences will be combined into a coherent, cleaned text while preserving the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def remove_redundant_sentences(text, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Removes redundant or highly similar sentences from the given text, preserving formatting such as \\n and spaces, while keeping key sentences.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text containing potentially redundant sentences.\n",
    "        similarity_threshold (float): The cosine similarity threshold above which\n",
    "                                       sentences are considered redundant.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with redundant sentences removed.\n",
    "    \"\"\"\n",
    "    # Split the text into sentences while preserving the delimiters and formatting\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    # Vectorize the sentences using TF-IDF\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "\n",
    "    # Compute cosine similarity between all sentence pairs\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "\n",
    "    # Identify sentences to keep\n",
    "    sentences_to_keep = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Check if the sentence is similar to any previously kept sentence\n",
    "        if all(similarity_matrix[i, j] < similarity_threshold for j in sentences_to_keep):\n",
    "            sentences_to_keep.append(i)\n",
    "\n",
    "    # Reconstruct the text with only unique sentences\n",
    "    unique_sentences = [sentences[i] for i in sentences_to_keep]\n",
    "    return '\\n'.join(unique_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "What is the cat's food ? Make me a short and clear list of the answers.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "The cat's food is the cat's favorite food.\n",
      "\n",
      "\n",
      "\n",
      "Cleaned Text:\n",
      "\n",
      "What is the cat's food ?\n",
      "Make me a short and clear list of the answers.\n",
      "The cat's food is the cat's favorite food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = remove_redundant_sentences(text_complete_sentences)\n",
    "\n",
    "print(\"Original Text:\\n\")\n",
    "print(generated_text)\n",
    "print(\"\\nCleaned Text:\\n\")\n",
    "print(cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
