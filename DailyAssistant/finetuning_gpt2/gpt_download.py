import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer


def download_gpt2(save_dir, model_size):
    """
    T√©l√©charge un mod√®le GPT-2 si les fichiers n√©cessaires ne sont pas d√©j√† pr√©sents.
    Si le mod√®le est d√©j√† t√©l√©charg√©, il est charg√© depuis les fichiers locaux.

    Args:
        model_size (str): Taille du mod√®le √† t√©l√©charger (e.g., 'gpt2', 'gpt2-medium').
        save_dir (str): R√©pertoire o√π le mod√®le sera sauvegard√©.

    Returns:
        tuple: (model, tokenizer) GPT-2 charg√© depuis le disque ou t√©l√©charg√©.
    """
    # Chemins des fichiers du mod√®le
    config_path = os.path.join(save_dir, "config.json")
    model_path = os.path.join(save_dir, "model.safetensors")  # Remis en .bin (Hugging Face format)
    tokenizer_path = os.path.join(save_dir, "vocab.json")

    # V√©rifier si le mod√®le est d√©j√† pr√©sent
    if os.path.exists(config_path) and os.path.exists(model_path) and os.path.exists(tokenizer_path):
        print(f"‚úÖ Le mod√®le {model_size} est d√©j√† pr√©sent dans {save_dir}, chargement depuis les fichiers locaux...")
        model = GPT2LMHeadModel.from_pretrained(save_dir).to("cuda" if torch.cuda.is_available() else "cpu")
        tokenizer = GPT2Tokenizer.from_pretrained(save_dir)
        return model, tokenizer

    # Cr√©er le r√©pertoire si n√©cessaire
    os.makedirs(save_dir, exist_ok=True)

    # T√©l√©charger et sauvegarder le mod√®le et le tokenizer
    print(f"üì• T√©l√©chargement du mod√®le {model_size} dans {save_dir}...")
    tokenizer = GPT2Tokenizer.from_pretrained(model_size)
    model = GPT2LMHeadModel.from_pretrained(model_size).to("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer.save_pretrained(save_dir)
    model.save_pretrained(save_dir)
    print(f"‚úÖ Mod√®le {model_size} t√©l√©charg√© et sauvegard√© dans {save_dir}.")

    return model, tokenizer
