{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HOW TO GENERATE A DATASET ABOUT COOKING ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why this method ?**\n",
    "\n",
    "Using Phi-2 for dataset generation is a strategic choice for our student project, given our limited resources and small team of three. As students, we lack the financial means to access large-scale AI infrastructure, so we must find a balance between quantity and quality to ensure we have enough data to fine-tune our model effectively. Phi-2, being a lightweight yet capable language model, allows us to generate a high volume of structured instructional data without requiring access to expensive high-end GPUs. By carefully designing prompts and filtering the generated output, we aim to maintain quality while maximizing the number of examples, ensuring that our model is trained on a dataset that is both diverse and relevant. This approach enables us to push the limits of what can be achieved with minimal resources, making AI research accessible even to small student-led initiatives like ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why using a GPU ?**\n",
    "\n",
    "We used a GPU to speed up the dataset generation process. Since Phi-2 is a Transformer-based model, running it on a CPU would be significantly slower, making it impractical to generate a large dataset efficiently. The GPU allows for faster text generation, reducing waiting times and enabling us to produce more data in less time, which is crucial given our limited resources and need for scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's initialize Phi-2 :**\n",
    "\n",
    "To initialize Phi-2, we first load the tokenizer and model using Hugging Face's ```AutoTokenizer``` and ```AutoModelForCausalLM```. We specify ```torch.float16``` to reduce memory usage and improve efficiency, which is crucial for running the model smoothly. The ```device_map=\"auto\"``` setting automatically assigns the model to the available hardware, utilizing the GPU if present for faster processing. This setup ensures that Phi-2 is optimized for inference while keeping resource consumption manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ GPU disponible : True\n",
      "üíª Nom du GPU : NVIDIA GeForce RTX 3050\n",
      "üî¢ Nombre de GPUs disponibles : 1\n",
      "üìù CUDA Version : 12.1\n",
      "üöÄ Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# ‚úÖ V√©rifier que l'on utilise la bonne version de transformers\n",
    "import transformers\n",
    "assert transformers.__version__ >= \"4.43.0\", \"‚ö†Ô∏è Upgrade transformers: pip install transformers==4.43.0\"\n",
    "\n",
    "print(\"üöÄ GPU disponible :\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"üíª Nom du GPU :\", torch.cuda.get_device_name(0))\n",
    "    print(\"üî¢ Nombre de GPUs disponibles :\", torch.cuda.device_count())\n",
    "    print(\"üìù CUDA Version :\", torch.version.cuda)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Running on: {device}\")\n",
    "\n",
    "# ‚úÖ Charger le mod√®le Phi-3.5-mini-instruct avec les param√®tres recommand√©s\n",
    "model_name = \"phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = torch.compile(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(text):\n",
    "    \"\"\"\n",
    "    Extrait la partie / reponse du texte g√©n√©r√© en cherchant les marqueurs \"Output:\" ou \"##OUTPUT\".\n",
    "    Si aucun marqueur n'est trouv√©, la fonction retourne le texte complet nettoy√©.\n",
    "    \"\"\"\n",
    "    # On d√©finit plusieurs patterns pour g√©rer les diff√©rents formats\n",
    "    patterns = [\n",
    "        r\"Output:\\s*(.*)\",                      # pour \"Output:\"\n",
    "        r\"##\\s*OUTPUT\\s*(.*)\",                   # pour \"##OUTPUT\" (avec ou sans espace)\n",
    "        r\"A:\\s*(?:Question:)?\\s*(.*)\",           # pour \"A:\" ou \"A: Question:\"\n",
    "        r\"ANSWER:\\s*(?:Question:)?\\s*(.*)\",      # pour \"ANSWER:\" avec ou sans \"Question:\"\n",
    "        r\"Answer:\\s*(?:Question:)?\\s*(.*)\",      # pour \"Answer:\" avec ou sans \"Question:\"\n",
    "        r\"AI:\\s*(.*)\",                          # pour \"AI:\"\n",
    "        r\"Response:\\s*(.*)\"                     # pour \"Response:\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            answer = match.group(1).strip()\n",
    "            if answer:\n",
    "                return answer\n",
    "            \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_simple_answer(answer: str) -> bool:\n",
    "    \"\"\"\n",
    "    V√©rifie si la r√©ponse est simple.\n",
    "    On consid√®re qu'une r√©ponse est simple si :\n",
    "      - Elle contient moins de 40 mots.\n",
    "      - Elle ne contient pas de listes (pas de tirets, puces, ou de num√©rotation comme \"1. \").\n",
    "      - Elle ne se termine pas par un deux-points (\":\") indiquant une coupure.\n",
    "      - Elle se termine par une ponctuation finale (\".\", \"?\" ou \"!\"), sinon elle est consid√©r√©e comme incompl√®te.\n",
    "    \"\"\"\n",
    "    words = answer.split()\n",
    "    if len(words) > 40:\n",
    "        return False\n",
    "    # Rejeter si la r√©ponse commence par une num√©rotation (ex: \"1. \")\n",
    "    if re.match(r\"^\\d+\\.\\s+\", answer):\n",
    "        return False\n",
    "    if any(token in answer for token in [\"- \", \"‚Ä¢\", \"\\n- \", \"\\n‚Ä¢\"]):\n",
    "        return False\n",
    "    if answer.strip().endswith(\":\"):\n",
    "        return False\n",
    "    if not answer.strip().endswith((\".\", \"?\", \"!\")):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_incomplete_sentence(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Supprime la derni√®re portion de texte si elle ne forme pas une phrase compl√®te,\n",
    "    c'est-√†-dire si elle ne se termine pas par un point, un point d'interrogation ou un point d'exclamation.\n",
    "    \"\"\"\n",
    "    answer = answer.strip()\n",
    "    if not answer:\n",
    "        return answer\n",
    "    if answer[-1] not in \".?!\":\n",
    "        # Trouver la derni√®re occurrence d'une ponctuation finale dans la r√©ponse\n",
    "        last_period = answer.rfind('.')\n",
    "        last_question = answer.rfind('?')\n",
    "        last_exclamation = answer.rfind('!')\n",
    "        last_index = max(last_period, last_question, last_exclamation)\n",
    "        if last_index != -1:\n",
    "            # Conserver uniquement le texte jusqu'√† (et incluant) cette ponctuation\n",
    "            answer = answer[:last_index+1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_simple_question(question: str) -> bool:\n",
    "    \"\"\"\n",
    "    V√©rifie si la question est simple selon deux crit√®res :\n",
    "      - Elle se termine par un point d'interrogation.\n",
    "      - Son nombre de mots est compris entre 3 et 15 (crit√®re ajustable).\n",
    "    \"\"\"\n",
    "    words = question.split()\n",
    "    return question.endswith(\"?\") and 3 <= len(words) <= 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QUESTION GENERATION :**\n",
    "\n",
    "The `generate_question(word1, word2)` function is designed to create concise and practical cooking-related questions based on two randomly chosen words from a predefined vocabulary of culinary terms. This process ensures variety and relevance in the generated dataset. Below is a step-by-step breakdown of how the function works:\n",
    "\n",
    "#### **1. Constructing the Prompt**\n",
    "The function first generates a structured prompt instructing the AI to create a short and useful cooking question. The prompt includes the two randomly selected words and sets clear expectations for the format and style of the generated question.\n",
    "\n",
    "#### **2. Text Generation Using a Pretrained Model**\n",
    "The prompt is tokenized and passed through a pre-trained language model. The model generates a response using specific parameters:\n",
    "   - `max_new_tokens=50` ensures a short output.\n",
    "   - `temperature=0.75` introduces controlled randomness.\n",
    "   - `top_p=0.9` allows more diverse responses.\n",
    "   - `do_sample=True` ensures non-deterministic sampling.\n",
    "\n",
    "This step produces a text output that may contain the expected question but can also include unwanted extra text.\n",
    "\n",
    "#### **3. Extracting the Question**\n",
    "To refine the output, the function calls `extract_text(text)`, which identifies and extracts the relevant portion of the generated text. This function applies multiple regex patterns to detect and isolate the question from the AI's response. It searches for markers like:\n",
    "   - `\"Output:\"`\n",
    "   - `\"##OUTPUT\"`\n",
    "   - `\"A:\"` or `\"Answer:\"`\n",
    "   - `\"AI:\"` or `\"Response:\"`\n",
    "\n",
    "If a match is found, the extracted portion is returned as the candidate question.\n",
    "\n",
    "#### **4. Validating the Question**\n",
    "The extracted text is then evaluated using `is_simple_question(question)`, which ensures that the generated question meets two criteria:\n",
    "   - **It must end with a question mark (`?`).**\n",
    "   - **It must contain between 3 and 15 words.**\n",
    "\n",
    "This step guarantees that the output is a well-formed, concise question rather than a lengthy explanation or an incomplete phrase.\n",
    "\n",
    "#### **5. Iterative Refinement**\n",
    "If the generated question does not meet the simplicity criteria, the function retries up to five times, regenerating the text with the same parameters until a suitable question is found.\n",
    "\n",
    "#### **6. Returning the Final Question**\n",
    "Once a valid question is obtained, it is returned in a dictionary format:\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"What are some ways to use garlic and butter in cooking?\",\n",
    "    \"response\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Fonction pour g√©n√©rer une question courte et unique\n",
    "def generate_question(word1, word2):\n",
    "    \"\"\"\n",
    "    G√©n√®re une courte question sur un th√®me donn√© de cuisine en relan√ßant la g√©n√©ration\n",
    "    tant que la question extraite n'est pas consid√©r√©e comme simple.\n",
    "    \n",
    "    Args:\n",
    "        word1 (str): Premier mot cl√© de cuisine.\n",
    "        word2 (str): Deuxi√®me mot cl√© de cuisine.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant uniquement l'instruction (la question simple) et une r√©ponse vide.\n",
    "    \"\"\"\n",
    "    prompt = f\"You are a friendly and concise cooking assistant. Your job is to generate a short, simple, and practical cooking question with few words based on two randomly selected ingredients or kitchen items :  {word1}, {word2}.\"\n",
    "    \n",
    "    max_attempts = 5  # Limite pour √©viter une boucle infinie\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        # üî• Tokenisation et g√©n√©ration\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=50,  # ‚úÖ Assure une question courte\n",
    "            temperature=0.75,   # ‚úÖ Ajoute un peu de diversit√©\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "        # üìú D√©codage du texte g√©n√©r√©\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "        # Extraction de la partie question √† partir des diff√©rents patterns\n",
    "        question = extract_text(generated_text)\n",
    "    \n",
    "        # Si la question est simple, on la renvoie\n",
    "        if is_simple_question(question):\n",
    "            return {\"instruction\": question, \"response\": \"\"}\n",
    "    \n",
    "    # Si apr√®s max_attempts aucune question simple n'est obtenue, on renvoie la derni√®re version\n",
    "    return {\"instruction\": question, \"response\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Temporary save at 100 questions.\n",
      "üíæ Temporary save at 200 questions.\n",
      "üíæ Temporary save at 300 questions.\n",
      "üíæ Temporary save at 400 questions.\n",
      "üíæ Temporary save at 500 questions.\n",
      "üíæ Temporary save at 600 questions.\n",
      "üíæ Temporary save at 700 questions.\n",
      "üíæ Temporary save at 800 questions.\n",
      "üíæ Temporary save at 900 questions.\n",
      "üíæ Temporary save at 1000 questions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# ‚úÖ G√©n√©rer 100 questions sur des th√®mes al√©atoires avec sauvegarde temporaire\n",
    "dataset = []\n",
    "used_questions = set()  # Utilisation d'un set pour stocker uniquement les questions sous forme de texte\n",
    "\n",
    "# Lire la liste de mots depuis le fichier\n",
    "with open(\"cooking_words_2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    cooking_words = file.read().splitlines()\n",
    "\n",
    "while len(dataset) < 1000:\n",
    "    word1, word2 = random.sample(cooking_words, 2)\n",
    "    question_data = generate_question(word1, word2)  # ‚úÖ R√©cup√®re un dictionnaire {\"instruction\": \"...\", \"response\": \"\"}\n",
    "\n",
    "    question_text = question_data[\"instruction\"]  # ‚úÖ Prend uniquement la question sous forme de string\n",
    "\n",
    "    # ‚úÖ V√©rifie que la question est unique et valide\n",
    "    if question_text not in used_questions:\n",
    "        dataset.append({\"instruction\": question_text, \"response\": \"\"})\n",
    "         #print(f\"‚úî Generated: {question_text}\")\n",
    "\n",
    "        # ‚úÖ Sauvegarde temporaire tous les 10 questions\n",
    "        if len(dataset) % 100 == 0:  # ‚úÖ Correction de la condition (100 ‚Üí 10)\n",
    "            with open(\"V5_temp_cooking_questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"üíæ Temporary save at {len(dataset)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset successfully saved as 'cooking_questions_dataset.json' üéâ\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Sauvegarde du dataset en JSON\n",
    "with open(\"cooking_questions_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n‚úÖ Dataset successfully saved as 'cooking_questions_dataset.json' üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QUESTION GENERATION :**\n",
    "\n",
    "The `generate_answer(question)` function is designed to create **concise, direct, and practical** cooking-related answers based on a given question. Unlike a full recipe or ingredient list, the responses generated are short and informative, making them ideal for quick culinary advice. Below is a step-by-step breakdown of how the function ensures high-quality output:\n",
    "\n",
    "### **1. Constructing the Prompt**\n",
    "The function creates a structured **prompt** instructing the AI to act as a knowledgeable yet concise cooking assistant. The prompt explicitly specifies that:\n",
    "   - The response should be **short** and **to the point**.\n",
    "   - It should **not contain a full recipe** or ingredient list.\n",
    "   - The response must be **practical and relevant** to the given question.\n",
    "\n",
    "Example Prompt:\n",
    "```text\n",
    "Act like a cooking chief. You are a friendly and concise cooking assistant. \n",
    "Your job is to generate a short, simple, and practical cooking answer with few words based on this question:  \n",
    "\"How can I make my scrambled eggs creamier?\"\n",
    "Do not provide a detailed recipe, do not list ingredients and write a simple and short answer.\n",
    "```\n",
    "\n",
    "### **2. Generating the Answer**\n",
    "The function then **tokenizes** the prompt and processes it through a pre-trained language model. The generation parameters ensure a well-structured and diverse response:\n",
    "   - `max_new_tokens=70` limits response length.\n",
    "   - `temperature=0.75` introduces moderate randomness.\n",
    "   - `top_p=0.9` filters out unlikely outputs.\n",
    "   - `do_sample=True` ensures non-deterministic responses.\n",
    "\n",
    "The model produces a raw text output that **may contain unwanted details or incomplete sentences**.\n",
    "\n",
    "\n",
    "### **3. Extracting and Cleaning the Answer**\n",
    "Once generated, the text is processed using **`extract_text(text)`** to isolate the relevant part of the AI response. This function:\n",
    "   - Searches for markers like `\"Answer:\"`, `\"Response:\"`, or `\"AI:\"` to identify the start of the answer.\n",
    "   - Removes any unnecessary leading or trailing text.\n",
    "\n",
    "The answer is then refined using **`clean_incomplete_sentence(answer)`**, which:\n",
    "   - Ensures the response ends with **proper punctuation (. ? !)**.\n",
    "   - If the last sentence is incomplete, it is removed to maintain clarity.\n",
    "\n",
    "\n",
    "### **4. Validating the Answer**\n",
    "To ensure the generated response meets quality standards, the function calls **`is_simple_answer(answer)`**, which checks:\n",
    "   - **Word count ‚â§ 40** (concise response).\n",
    "   - **No lists or bullet points** (`- `, `‚Ä¢`, `1.` are rejected).\n",
    "   - **No incomplete phrases** (must end with `.`, `?`, or `!`).\n",
    "   - **No trailing colons (`:`)**, which may indicate a cutoff response.\n",
    "\n",
    "If the generated answer **fails validation**, the function retries up to **five times**, ensuring only well-formed answers are returned.\n",
    "\n",
    "\n",
    "### **5. Returning the Final Answer**\n",
    "Once a valid response is obtained, it is returned in JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"How can I make my scrambled eggs creamier?\",\n",
    "    \"response\": \"Add a splash of heavy cream and cook on low heat while stirring continuously.\"\n",
    "}\n",
    "```\n",
    "\n",
    "If no valid answer is generated within the maximum retries, the last attempt is returned.\n",
    "\n",
    "\n",
    "### **Why This Method is Effective?**\n",
    "‚úî **Ensures concise, well-structured answers** without unnecessary details.  \n",
    "‚úî **Retries up to 5 times** to improve answer quality.  \n",
    "‚úî **Filters out incomplete, overly long, or poorly formatted responses.**  \n",
    "‚úî **Creates useful, digestible knowledge for AI-powered cooking assistants.**  \n",
    "\n",
    "By combining structured **prompting**, **post-processing**, and **quality checks**, this method produces **high-quality cooking advice**, making it suitable for **AI training datasets, chatbots, and virtual assistants**. üç≥üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str):\n",
    "    \"\"\"\n",
    "    G√©n√®re une r√©ponse concise (2-3 lignes maximum, sans listes ni recettes d√©taill√©es)\n",
    "    pour la question donn√©e. Si la r√©ponse extraite n'est pas consid√©r√©e comme simple\n",
    "    (par exemple, si elle est incompl√®te et ne se termine pas par une ponctuation finale),\n",
    "    la g√©n√©ration est relanc√©e jusqu'√† un maximum d'essais.\n",
    "    \n",
    "    Args:\n",
    "        question (str): La question √† laquelle r√©pondre.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant la question dans \"instruction\" et la r√©ponse dans \"response\".\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Act like a cooking chief. You are a friendly and concise cooking assistant. \"\n",
    "        f\"Your job is to generate a short, simple, and practical cooking answer with few words based on this question \\\"{question}\\\".\\n\"\n",
    "        f\"Do not provide a detailed recipe, do not list ingredients and write a simple and short answer.\"\n",
    "    )\n",
    "    \n",
    "    max_attempts = 5  # Limite pour √©viter une boucle infinie\n",
    "    attempt = 0\n",
    "    reponse = \"\"\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=70,\n",
    "            temperature=0.75,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id         \n",
    "        )         \n",
    "                 \n",
    "        # D√©codage du texte g√©n√©r√©         \n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "        reponse = extract_text(generated_text)\n",
    "        \n",
    "        # Nettoyer la r√©ponse pour supprimer la derni√®re phrase incompl√®te\n",
    "        reponse = clean_incomplete_sentence(reponse)\n",
    "        \n",
    "        # V√©rifie si la r√©ponse est simple et compl√®te\n",
    "        if is_simple_answer(reponse):\n",
    "            return {\"instruction\": question, \"response\": reponse}\n",
    "        # else:\n",
    "        #     print(f\"Attempt {attempt}: Answer not simple enough, regenerating... Generated answer: {reponse}\")\n",
    "    \n",
    "    #print(\"Max attempts reached. Returning the last generated answer.\")\n",
    "    return {\"instruction\": question, \"response\": reponse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary save at 100 questions processed.\n",
      "Temporary save at 200 questions processed.\n",
      "Temporary save at 300 questions processed.\n",
      "Temporary save at 400 questions processed.\n",
      "Temporary save at 500 questions processed.\n",
      "Temporary save at 600 questions processed.\n",
      "Temporary save at 700 questions processed.\n",
      "Temporary save at 800 questions processed.\n",
      "Temporary save at 900 questions processed.\n",
      "Temporary save at 1000 questions processed.\n",
      "La g√©n√©ration de r√©ponses est termin√©e et le fichier a √©t√© sauvegard√© sous 'cooking_questions_with_answers.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Charger le dataset existant\n",
    "with open(\"V5_temp_cooking_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# It√©rer sur chaque entr√©e et g√©n√©rer la r√©ponse\n",
    "for i, entry in enumerate(dataset):\n",
    "    question = entry[\"instruction\"]\n",
    "    # print(f\"Processing question {i+1}/{len(dataset)}:\")\n",
    "    # print(f\"Question: {question}\")\n",
    "    \n",
    "    # G√©n√©rer la r√©ponse pour la question donn√©e\n",
    "    answer_data = generate_answer(question)\n",
    "    answer = answer_data[\"response\"]\n",
    "    \n",
    "    # Mettre √† jour l'entr√©e avec la r√©ponse g√©n√©r√©e\n",
    "    entry[\"response\"] = answer\n",
    "    # print(f\"Generated answer: {answer}\")\n",
    "    # print(\"-\" * 50)\n",
    "    \n",
    "    # Sauvegarde temporaire toutes les 100 questions\n",
    "    if (i + 1) % 100 == 0:\n",
    "        with open(\"V5_temp_cooking_questions_with_answers.json\", \"w\", encoding=\"utf-8\") as f_temp:\n",
    "            json.dump(dataset, f_temp, indent=4, ensure_ascii=False)\n",
    "        print(f\"Temporary save at {i+1} questions processed.\")\n",
    "\n",
    "# Sauvegarder le dataset complet dans un nouveau fichier JSON\n",
    "with open(\"V5_cooking_questions_with_answers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"La g√©n√©ration de r√©ponses est termin√©e et le fichier a √©t√© sauvegard√© sous 'cooking_questions_with_answers.json'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
