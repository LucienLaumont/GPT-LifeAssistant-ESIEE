{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; color:rgb(255, 255, 255);\">Final Project: Building a Daily Life Assistant</h1>\n",
    "\n",
    "<p style=\"font-size: 25px; line-height: 1.6; text-align: justify; max-width: 1200px; margin: 0 auto; margin-bottom: 20px;\">\n",
    "    This project aims to create a practical AI assistant using an instruction fine-tuned GPT-2 model. \n",
    "    The assistant will perform daily tasks such as scheduling, answering questions, and providing personalized recommendations.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size: 20px; line-height: 1.8; max-width: 1000px; margin: 0 auto;\">\n",
    "    <li><strong>Model Architecture & Pretraining</strong>: Understanding GPT-2‚Äôs architecture and pretraining process.</li>\n",
    "    <li><strong>Instruction Fine-Tuning</strong>: Training the model with instruction-response pairs for enhanced task performance.</li>\n",
    "    <li><strong>Evaluation & Refinement</strong>: Assessing the model's output and iterating for better results.</li>\n",
    "    <li><strong>Practical Application</strong>: Implementing the model in real-world scenarios such as daily task management.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "We start by loading the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucie\\Documents\\Cours\\E5\\llm\\GPT-LifeAssistant-ESIEE\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "from gpt_download import download_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the Pretrained GPT-2 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\")\n",
    "MODEL_SIZE = os.getenv(\"MODEL_SIZE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement du mod√®le gpt2-large dans model...\n",
      "Mod√®le gpt2-large t√©l√©charg√© et sauvegard√© dans model.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = download_gpt2(MODEL_DIR,MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of Spain ?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the capital of Spain ?\\n\\nThe capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    attention_mask = attention_mask,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    max_length = input_ids.shape[1] + 100,\n",
    "    num_beams = 5,\n",
    "    temperature = 1,\n",
    "    top_k = 50,\n",
    "    do_sample = True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Incomplete Responses from GPT-2**\n",
    "\n",
    "In some cases, GPT-2 provides an incomplete response. It starts a sentence but doesn't finish it. This is problematic because it negatively impacts the user experience. There are several solutions to address this issue:\n",
    "\n",
    "1. **Fine-tuning with examples of complete responses**: Train the model on a dataset that includes well-structured and complete answers to improve its behavior.\n",
    "\n",
    "2. **Post-processing generated responses**: Implement logic to analyze the output and request the model to continue if a response is detected as incomplete.\n",
    "\n",
    "3. **Adjusting generation parameters**: Modify parameters such as `max_length`, `temperature`, `top_k`, or `top_p` to increase the likelihood of producing complete and coherent answers.\n",
    "\n",
    "4. **Adding a contextual prefix**: Use a prompt like _\"Please provide a detailed and complete answer:\"_ before the main query to guide the model towards better responses.\n",
    "\n",
    "5. **Automatic verification with a script**: Create a script to detect incomplete responses and prompt the model to continue if necessary.\n",
    "\n",
    "\n",
    "We will choose to integrate a script to handle this issue. Additionally, during the fine-tuning process for a daily assistant, we will ensure that this concern is addressed in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_incomplete_sentences(text):\n",
    "    \"\"\"\n",
    "    Slice the input text into sentences, keeping the formatting \n",
    "    (e.g., \\n, spaces), and remove incomplete phrases that do not \n",
    "    end with a proper punctuation mark.\n",
    "    \"\"\"\n",
    "    # Split the text while keeping the delimiters (e.g., .!?) and formatting\n",
    "    sentences = re.split(r'(?<=[.!?])(\\s+)', text)\n",
    "    \n",
    "    cleaned_text = \"\"\n",
    "    for i in range(0, len(sentences) - 1, 2):  # Process sentences with their trailing spaces\n",
    "        sentence = sentences[i]\n",
    "        trailing_space = sentences[i + 1]\n",
    "        if re.search(r'[.!?]$', sentence):  # Check if the sentence ends with valid punctuation\n",
    "            cleaned_text += sentence + trailing_space\n",
    "    \n",
    "    # Handle cases where the last part is an incomplete sentence\n",
    "    if len(sentences) % 2 != 0 and re.search(r'[.!?]$', sentences[-1]):\n",
    "        cleaned_text += sentences[-1]\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "What is the capital of Spain ?\n",
      "\n",
      "The capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid, the capital of Spain is Madrid,\n",
      "\n",
      "Cleaned Text:\n",
      "\n",
      "What is the capital of Spain ?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_complete_sentences = clean_incomplete_sentences(generated_text)\n",
    "\n",
    "print(\"Original Text:\\n\")\n",
    "print(generated_text)\n",
    "print(\"\\nCleaned Text:\\n\")\n",
    "print(text_complete_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Redundant Sentences in GPT-Generated Text**\n",
    "\n",
    "GPT often generates sentences that are almost identical or convey similar information. This redundancy can make it challenging to filter out phrases with overlapping content. To address this issue, we will implement a script that detects and removes duplicate or nearly identical sentences.\n",
    "\n",
    "1. **Sentence Splitting**: \n",
    "   - The text will be divided into individual sentences using a delimiter (e.g., `.`, `!`, `?`).\n",
    "\n",
    "2. **Similarity Detection**:\n",
    "   - We will compare each sentence against others using a similarity metric, such as Levenshtein distance or cosine similarity on vector embeddings.\n",
    "\n",
    "3. **Duplicate Removal**:\n",
    "   - Sentences identified as duplicates or with high similarity scores will be removed, leaving only unique information.\n",
    "\n",
    "4. **Reconstruction**:\n",
    "   - The remaining unique sentences will be combined into a coherent, cleaned text while preserving the original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def remove_redundant_sentences(text, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Removes redundant or highly similar sentences from the given text, preserving formatting such as \\n and spaces, while keeping key sentences.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text containing potentially redundant sentences.\n",
    "        similarity_threshold (float): The cosine similarity threshold above which\n",
    "                                       sentences are considered redundant.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with redundant sentences removed.\n",
    "    \"\"\"\n",
    "    # Split the text into sentences while preserving the delimiters and formatting\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    # Vectorize the sentences using TF-IDF\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "\n",
    "    # Compute cosine similarity between all sentence pairs\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "\n",
    "    # Identify sentences to keep\n",
    "    sentences_to_keep = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Check if the sentence is similar to any previously kept sentence\n",
    "        if all(similarity_matrix[i, j] < similarity_threshold for j in sentences_to_keep):\n",
    "            sentences_to_keep.append(i)\n",
    "\n",
    "    # Reconstruct the text with only unique sentences\n",
    "    unique_sentences = [sentences[i] for i in sentences_to_keep]\n",
    "    return '\\n'.join(unique_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "What is the capital of Spain ?\n",
      "\n",
      "\n",
      "\n",
      "Cleaned Text:\n",
      "\n",
      "What is the capital of Spain ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = remove_redundant_sentences(text_complete_sentences)\n",
    "\n",
    "print(\"Original Text:\\n\")\n",
    "print(text_complete_sentences)\n",
    "print(\"\\nCleaned Text:\\n\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finetune The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lucie\\Documents\\Cours\\E5\\llm\\GPT-LifeAssistant-ESIEE\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "with open('prompt.json', 'r',encoding='utf8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the dataset\n",
    "def preprocess(example):\n",
    "    # Combine instruction, input, and output into a single text prompt\n",
    "    prompt = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example['input']:\n",
    "        prompt += f\"Input: {example['input']}\\n\"\n",
    "    prompt += f\"Output: {example['output']}\"\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [00:00<00:00, 6160.69 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 4005.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 3: Split the dataset into training and evaluation sets\n",
    "train_data, eval_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert train and eval data into Hugging Face Datasets\n",
    "train_dataset = Dataset.from_list(train_data).map(preprocess)\n",
    "eval_dataset = Dataset.from_list(eval_data).map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [00:00<00:00, 1321.69 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 993.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Tokenize the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 does not have a pad token\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucie\\Documents\\Cours\\E5\\llm\\GPT-LifeAssistant-ESIEE\\env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucie\\AppData\\Local\\Temp\\ipykernel_32660\\3004567566.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Train the model\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save the fine-tuned model\n",
    "\n",
    "# model.save_pretrained(\"./model_finetuned\")\n",
    "# tokenizer.save_pretrained(\"./model_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
