
import os

MODEL_DIR = os.getenv("MODEL_DIR", "model")  # Par d√©faut, sauvegarde dans "model"
MODEL_SIZE = os.getenv("MODEL_SIZE", "gpt2")  # Par d√©faut, utilise "gpt2"

# D√©sactiver certains avertissements TensorFlow et Hugging Face
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

from transformers import GPT2LMHeadModel, GPT2Tokenizer


# V√©rifier si le mod√®le est d√©j√† t√©l√©charg√©, sinon le r√©cup√©rer
def download_gpt2(save_dir, model_size):
    """
    T√©l√©charge un mod√®le GPT-2 si les fichiers n√©cessaires ne sont pas d√©j√† pr√©sents.

    Args:
        model_size (str): Taille du mod√®le √† t√©l√©charger (e.g., 'gpt2', 'gpt2-medium').
        save_dir (str): R√©pertoire o√π le mod√®le sera sauvegard√©.

    Returns:
        tuple: (model, tokenizer) GPT-2
    """
    config_path = os.path.join(save_dir, "config.json")
    model_path = os.path.join(save_dir, "model.safetensors")
    tokenizer_path = os.path.join(save_dir, "vocab.json")

    # V√©rifier si un mod√®le fine-tun√© existe d√©j√†
    if os.path.exists(config_path) and os.path.exists(model_path) and os.path.exists(tokenizer_path):
        print(f"‚úÖ Mod√®le fine-tun√© trouv√© dans {save_dir}, chargement...")
        model = GPT2LMHeadModel.from_pretrained(save_dir)
        tokenizer = GPT2Tokenizer.from_pretrained(save_dir)
    else:
        # T√©l√©charger le mod√®le si aucun mod√®le fine-tun√© n'est pr√©sent
        os.makedirs(save_dir, exist_ok=True)
        print(f"üì• T√©l√©chargement du mod√®le {model_size} ...")
        tokenizer = GPT2Tokenizer.from_pretrained(model_size)
        model = GPT2LMHeadModel.from_pretrained(model_size)
        tokenizer.save_pretrained(save_dir)
        model.save_pretrained(save_dir)
        print(f"‚úÖ Mod√®le {model_size} t√©l√©charg√© et sauvegard√© dans {save_dir}.")

    return model, tokenizer
